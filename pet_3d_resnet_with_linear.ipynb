{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T01:38:37.283771Z",
     "start_time": "2020-02-18T01:38:36.679527Z"
    }
   },
   "source": [
    "# PET Residual Neural Network with Linear Layer\n",
    "This is the resnet model with a linear layer at the end; data is padded with black borders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:23:01.176689Z",
     "start_time": "2020-03-02T13:23:00.108805Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "from sklearn import preprocessing\n",
    "from torchvision import transforms, utils\n",
    "import adabound\n",
    "import numpy as np\n",
    "\n",
    "import nibabel as nib\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:23:01.212484Z",
     "start_time": "2020-03-02T13:23:01.178565Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the GPU if there is one, otherwise CPU\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "CSV_DIR = \"./scores.csv\"\n",
    "DATA_DIR = \"./pet_data/\"\n",
    "\n",
    "MIN = 4401596\n",
    "MAX = 9233460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:23:01.279584Z",
     "start_time": "2020-03-02T13:23:01.216011Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# %load resnet.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "__all__ = [\n",
    "    'ResNet', 'resnet10', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "    'resnet152', 'resnet200'\n",
    "]\n",
    "\n",
    "\n",
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    # 3x3x3 convolution with padding\n",
    "    return nn.Conv3d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=1,\n",
    "        bias=False)\n",
    "\n",
    "\n",
    "def downsample_basic_block(x, planes, stride):\n",
    "    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "    zero_pads = torch.Tensor(\n",
    "        out.size(0), planes - out.size(1), out.size(2), out.size(3),\n",
    "        out.size(4)).zero_()\n",
    "    if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "        zero_pads = zero_pads.cuda()\n",
    "\n",
    "    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 layers,\n",
    "                 sample_size,\n",
    "                 sample_duration,\n",
    "                 shortcut_type='B',\n",
    "                 num_classes=400):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(\n",
    "            1,\n",
    "            64,\n",
    "            kernel_size=7,\n",
    "            stride=(2, 2, 1),\n",
    "            padding=(3, 3, 3),\n",
    "            bias=False)\n",
    "        self.bn1 = nn.InstanceNorm3d(1)\n",
    "        self.relu = nn.ELU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n",
    "        self.layer2 = self._make_layer(\n",
    "            block, 128, layers[1], shortcut_type, stride=2)\n",
    "        self.layer3 = self._make_layer(\n",
    "            block, 256, layers[2], shortcut_type, stride=2)\n",
    "        self.layer4 = self._make_layer(\n",
    "            block, 512, layers[3], shortcut_type, stride=2)\n",
    "        last_duration = int(math.ceil(sample_duration / 16))\n",
    "        last_size = int(math.ceil(sample_size / 32))\n",
    "        self.avgpool = nn.AvgPool3d(\n",
    "            (last_duration, last_size, last_size), stride=1)                \n",
    "        \n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            if shortcut_type == 'A':\n",
    "                downsample = partial(\n",
    "                    downsample_basic_block,\n",
    "                    planes=planes * block.expansion,\n",
    "                    stride=stride)\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv3d(\n",
    "                        self.inplanes,\n",
    "                        planes * block.expansion,\n",
    "                        kernel_size=1,\n",
    "                        stride=stride,\n",
    "                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_fine_tuning_parameters(model, ft_begin_index):\n",
    "    if ft_begin_index == 0:\n",
    "        return model.parameters()\n",
    "\n",
    "    ft_module_names = []\n",
    "    for i in range(ft_begin_index, 5):\n",
    "        ft_module_names.append('layer{}'.format(i))\n",
    "    ft_module_names.append('fc')\n",
    "\n",
    "    parameters = []\n",
    "    for k, v in model.named_parameters():\n",
    "        for ft_module in ft_module_names:\n",
    "            if ft_module in k:\n",
    "                parameters.append({'params': v})\n",
    "                break\n",
    "        else:\n",
    "            parameters.append({'params': v, 'lr': 0.0})\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def resnet10(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [1, 1, 1, 1], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet200(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 24, 36, 3], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:23:01.306591Z",
     "start_time": "2020-03-02T13:23:01.281686Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_DIR)\n",
    "norm_df = df[[\"mmse\", \"cdr\", \"ageAtEntry\"]]\n",
    "\n",
    "std_scale = preprocessing.StandardScaler().fit(norm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Management\n",
    "Handle CSV diagnosis/signs and get an iterator of brain scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:23:01.317246Z",
     "start_time": "2020-03-02T13:23:01.310182Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_scores(ID, date):\n",
    "    scores = []\n",
    "    for index, row in df[df[\"Subject\"].str.contains(ID)].iterrows():\n",
    "        cur_date = int(row[\"ADRC_ADRCCLINICALDATA ID\"].split(\"_\")[-1][1:])\n",
    "        if cur_date > date:\n",
    "            if cur_date > date:\n",
    "                if pd.isna(row[\"mmse\"]): row[\"mmse\"] = 30\n",
    "                if pd.isna(row[\"cdr\"]): row[\"cdr\"] = 0\n",
    "                data = {\n",
    "                    'mmse':  [row[\"mmse\"]],\n",
    "                    'cdr':  [row[\"cdr\"]],\n",
    "                    'ageAtEntry': [row[\"ageAtEntry\"]+cur_date/365]\n",
    "                }\n",
    "\n",
    "                curr_df = std_scale.transform(pd.DataFrame(data, columns=[\"mmse\", \"cdr\", \"ageAtEntry\"]))\n",
    "\n",
    "                scores.append((cur_date-date, curr_df[0][0], curr_df[0][1], curr_df[0][2]))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:23:01.351643Z",
     "start_time": "2020-03-02T13:23:01.318638Z"
    }
   },
   "outputs": [],
   "source": [
    "get_scores('OAS30001', 423)  # testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:23:01.362326Z",
     "start_time": "2020-03-02T13:23:01.353533Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_brains():\n",
    "    subjects = range(1, 11173)\n",
    "    for subject in subjects:\n",
    "        subject_id = str(subject).zfill(4)\n",
    "        path = f\"{DATA_DIR}sub-OAS3{subject_id}/\"\n",
    "        if os.path.isdir(path):\n",
    "            for session in os.listdir(path):\n",
    "                file = f\"{path}{session}/pet/sub-OAS3{subject_id}_{session}_acq-PIB_pet.nii.gz\"\n",
    "                if os.path.isfile(file):\n",
    "                    for score in get_scores(f\"OAS3{subject_id}\", int(session[5:])):\n",
    "                        yield (file, f\"OAS3{subject_id}\", int(session[5:])) + score\n",
    "                else:\n",
    "                    print(file)\n",
    "\n",
    "def list_brains():\n",
    "    subjects = range(1, 11173)\n",
    "    for subject in subjects:\n",
    "        subject_id = str(subject).zfill(4)\n",
    "        path = f\"{DATA_DIR}sub-OAS3{subject_id}/\"\n",
    "        if os.path.isdir(path):\n",
    "            for session in os.listdir(path):\n",
    "                file = f\"{path}{session}/pet/sub-OAS3{subject_id}_{session}_acq-PIB_pet.nii.gz\"\n",
    "                if os.path.isfile(file):\n",
    "                    yield (file, f\"OAS3{subject_id}\", int(session[5:]))\n",
    "                else:\n",
    "                    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset\n",
    "Create an iterable dataset with brain data inheriting from `torch.utils.data.Dataset`. Dataset len is `3594`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:23:01.581272Z",
     "start_time": "2020-03-02T13:23:01.572168Z"
    }
   },
   "outputs": [],
   "source": [
    "class BrainsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.brains = []\n",
    "        for brain_name in get_brains():\n",
    "            if get_scores(*brain_name[1:3]) != []:\n",
    "                self.brains.append(brain_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.brains)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = nib.load(self.brains[index][0])\n",
    "        return self.brains[index][3], self.brains[index][4], self.brains[index][5], self.brains[index][6], self.transform((data.get_fdata()+MIN)/MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T20:44:03.363326Z",
     "start_time": "2020-02-18T20:44:03.357375Z"
    }
   },
   "source": [
    "## Create Data Preprocessing and Cropping\n",
    "Crop images to (128, 128, 63, 24)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:23:02.202191Z",
     "start_time": "2020-03-02T13:23:02.193625Z"
    }
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, brain):\n",
    "        img = transform.resize(brain, self.output_size)\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return torch.from_numpy(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network\n",
    "Create a sparse cnn module inheriting from `torch.nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:23:02.980398Z",
     "start_time": "2020-03-02T13:23:02.965151Z"
    }
   },
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "\n",
    "    def forward(self, input):\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "\n",
    "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.num_classes = 64\n",
    "        self.resnet = resnet18(sample_size=10, sample_duration=10, num_classes=self.num_classes).to(DEVICE)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.num_classes*24+2, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            torch.nn.ELU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            torch.nn.ELU(),\n",
    "            nn.Linear(128, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            torch.nn.ELU(),\n",
    "            nn.Linear(16, 2)\n",
    "        ).to(DEVICE)\n",
    "\n",
    "    def forward(self, brain, days_ahead, age):\n",
    "        c_out = self.resnet(brain[:, None, :, :, 0])\n",
    "        for i in range(brain.shape[-1]-1):\n",
    "            _slice = brain[:, None, :, :, i]\n",
    "            c_out = torch.cat([c_out, self.resnet(_slice)], 1)\n",
    "        c_out = torch.cat([torch.stack([days_ahead, age]).permute(1, 0), c_out], 1)\n",
    "        r_out = torch.cuda.FloatTensor(self.linear(c_out)).to(DEVICE)\n",
    "        return r_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:23:03.425520Z",
     "start_time": "2020-03-02T13:23:03.399196Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, criterion_test, train_loader, test_loader, writer):\n",
    "    for epoch in range(4, 100):\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        print('-' * 10)\n",
    "        running_loss = 0\n",
    "        train_iter = iter(train_loader)\n",
    "        for i, data_brains in enumerate(train_loader):\n",
    "            scan = data_brains[4].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "            days_ahead = data_brains[0].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "            age = data_brains[3].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "            real_values = torch.stack([data_brains[1], data_brains[2]]).permute(1, 0).to(DEVICE)\n",
    "\n",
    "            outputs = model(scan, days_ahead, age)\n",
    "\n",
    "            loss = criterion(outputs, real_values)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 5 == 4:\n",
    "                print(f\"[{epoch + 1} {i + 1}] loss: {running_loss/5}\")\n",
    "                writer.add_scalar(\"Training Loss\", running_loss/5, i*3*(epoch+1))\n",
    "                running_loss = 0\n",
    "\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    try:\n",
    "                        test_data = next(train_iter)\n",
    "                    except StopIteration:\n",
    "                        train_iter = iter(train_loader)\n",
    "                        test_data = next(train_iter)\n",
    "\n",
    "                    _scan = test_data[4].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "                    _days_ahead = test_data[0].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "                    _age = test_data[3].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "                    _real_values = torch.stack([test_data[1], test_data[2]]).permute(1, 0).to(DEVICE)\n",
    "                    _outputs = model(_scan, _days_ahead, _age)\n",
    "                    writer.add_scalar(\"Test Loss\", criterion_test(_outputs, _real_values), i*3*(epoch+1))\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        print(\"Saving model\")\n",
    "        torch.save(model, f\"new_model_{epoch}_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:23:04.594340Z",
     "start_time": "2020-03-02T13:23:04.590893Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:24:01.844541Z",
     "start_time": "2020-03-02T13:23:05.570153Z"
    }
   },
   "outputs": [],
   "source": [
    "scale = Rescale((128, 128, 63, 24))\n",
    "composed = transforms.Compose([scale, ToTensor()])\n",
    "\n",
    "dataset = BrainsDataset(composed)\n",
    "\n",
    "NUM_INSTANCES = len(dataset)\n",
    "TEST_RATIO = 0.2\n",
    "TEST_SIZE = int(NUM_INSTANCES * TEST_RATIO)\n",
    "TRAIN_SIZE = NUM_INSTANCES - TEST_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:24:01.882811Z",
     "start_time": "2020-03-02T13:24:01.846968Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = torch.utils.data.random_split(dataset, (TRAIN_SIZE, TEST_SIZE))\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:24:01.893263Z",
     "start_time": "2020-03-02T13:24:01.886171Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"for data in train_loader:\n",
    "    data = data\n",
    "    break\n",
    "data\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T13:06:35.919762Z",
     "start_time": "2020-03-02T13:24:01.895730Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "model = torch.load(\"new_model_2_cnn.pt\").to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters())  \n",
    "criterion = torch.nn.MSELoss()  \n",
    "criterion_test = torch.nn.MSELoss()\n",
    "writer = SummaryWriter()\n",
    "\n",
    "loss = train(model, optimizer, criterion, criterion_test, train_loader, test_loader, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T13:06:39.376026Z",
     "start_time": "2020-03-05T13:06:39.359511Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T13:22:25.616Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, data_brains in enumerate(train_loader):\n",
    "    scan = data_brains[4].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "    days_ahead = data_brains[0].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "    age = data_brains[3].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "\n",
    "    writer.add_graph(model, (scan, days_ahead, age))\n",
    "    break\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T13:09:12.468063Z",
     "start_time": "2020-03-05T13:09:11.434653Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, f\"model_cnn_normal_final_new.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T13:22:25.632Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T13:22:25.638Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, data_brains in enumerate(train_loader):\n",
    "        # data_brains = days_ahead, mmse, cdr, age, scan\n",
    "        # output = (days ahead, mmse, cdr)\n",
    "        # scan = [x,y,z,t]\n",
    "        scan = data_brains[4].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "        days_ahead = data_brains[0].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "        # mmse = mmse.type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "        # cdr = cdr.type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "        age = data_brains[3].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "        real_values = torch.stack([data_brains[1], data_brains[2]]).permute(1, 0).to(DEVICE)\n",
    "\n",
    "        # print(scan.shape)\n",
    "        model.eval()\n",
    "        outputs = model(scan[None, 0], days_ahead[None, 0], age[None, 0])\n",
    "        print(f\"Outputs: {outputs}\")\n",
    "        print(f\"Real Values: {real_values}\")\n",
    "\n",
    "        data_pred = {\n",
    "            'mmse':  [outputs[0][0]],\n",
    "            'cdr':  [outputs[0][1]]\n",
    "        }\n",
    "\n",
    "        data_pred = std_scale.inverse_transform(pd.DataFrame(data_pred, columns=[\"mmse\", \"cdr\", \"ageAtEntry\"]))\n",
    "\n",
    "        data_real = {\n",
    "            'mmse':  [real_values[0][0]],\n",
    "            'cdr':  [real_values[0][1]]\n",
    "        }\n",
    "\n",
    "        data_real = std_scale.inverse_transform(pd.DataFrame(data_real, columns=[\"mmse\", \"cdr\", \"ageAtEntry\"]))\n",
    "    \n",
    "        print(data_pred)\n",
    "        print(data_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T13:22:25.650Z"
    }
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Dimensions of Data\n",
    "Here we get the dimensions of the data to be cropped\n",
    "\n",
    "This distribution goes something like this:\n",
    "\n",
    "```\n",
    "{(128, 128, 63, 51): 88,\n",
    " (128, 128, 63, 25): 60,\n",
    " (128, 128, 63, 24): 11,\n",
    " (128, 128, 109, 26): 549,\n",
    " (128, 128, 63, 52): 76,\n",
    " (128, 128, 63, 53): 41,\n",
    " (128, 128, 63, 26): 51,\n",
    " (128, 128, 63, 50): 34,\n",
    " (256, 256, 127, 26): 5,\n",
    " (128, 128, 63, 41): 2,\n",
    " (128, 128, 2592): 1,\n",
    " (128, 128, 74, 25): 1,\n",
    " (128, 128, 63, 49): 11,\n",
    " (256, 256, 127): 2,\n",
    " (128, 128, 63, 23): 4,\n",
    " (128, 128, 2832): 4,\n",
    " (128, 128, 63, 34): 2,\n",
    " (128, 128, 47, 49): 2,\n",
    " (128, 128, 63, 48): 1,\n",
    " (128, 128, 109, 4): 1,\n",
    " (128, 128, 2827): 1,\n",
    " (128, 128, 47, 50): 1,\n",
    " (128, 128, 109, 20): 2,\n",
    " (128, 128, 47, 52): 1,\n",
    " (128, 128, 109, 17): 1,\n",
    " (128, 128, 109, 6): 1,\n",
    " (128, 128, 63, 20): 1,\n",
    " (128, 128, 47, 51): 1,\n",
    " (128, 128, 63, 45): 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T13:22:25.653Z"
    }
   },
   "outputs": [],
   "source": [
    "dimensions = {}\n",
    "\n",
    "for brain in get_brains():\n",
    "    shape = nib.load(brain[0]).get_fdata().shape\n",
    "    if shape in dimensions:\n",
    "        dimensions[shape] += 1\n",
    "    else:\n",
    "        dimensions[shape] = 1\n",
    "\n",
    "print(dimensions)\n",
    "\n",
    "nums = 0\n",
    "for num in dimensions.values():\n",
    "    nums+=num\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T13:22:25.656Z"
    }
   },
   "outputs": [],
   "source": [
    "smallest, largest = 0, 0\n",
    "\n",
    "for brain in list_brains():\n",
    "    data = nib.load(brain[0]).get_fdata()\n",
    "    \n",
    "    if np.min(data) < smallest:\n",
    "        smallest = np.min(data)\n",
    "    if int(np.max(data)) > largest:\n",
    "        largest = np.max(data)\n",
    "\n",
    "(smallest, largest)\n",
    "# should be (-4401596.0, 4831864.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T13:22:25.662Z"
    }
   },
   "outputs": [],
   "source": [
    "for brain in get_brains():\n",
    "    shape = nib.load(brain[0]).get_fdata().shape\n",
    "    if len(shape) == 3:\n",
    "        print(brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T13:22:25.665Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_data = ['../data/sub-OAS30065/ses-d0553/pet/sub-OAS30065_ses-d0553_acq-PIB_pet.nii.gz',\n",
    "'../data/sub-OAS30229/ses-d0101/pet/sub-OAS30229_ses-d0101_acq-PIB_pet.nii.gz',\n",
    "'../data/sub-OAS30253/ses-d3948/pet/sub-OAS30253_ses-d3948_acq-PIB_pet.nii.gz',\n",
    "'../data/sub-OAS30332/ses-d0091/pet/sub-OAS30332_ses-d0091_acq-PIB_pet.nii.gz',\n",
    "'../data/sub-OAS30472/ses-d1278/pet/sub-OAS30472_ses-d1278_acq-PIB_pet.nii.gz',\n",
    "'../data/sub-OAS30498/ses-d0120/pet/sub-OAS30498_ses-d0120_acq-PIB_pet.nii.gz',\n",
    "'../data/sub-OAS30588/ses-d1639/pet/sub-OAS30588_ses-d1639_acq-PIB_pet.nii.gz',\n",
    "'../data/sub-OAS30896/ses-d1601/pet/sub-OAS30896_ses-d1601_acq-PIB_pet.nii.gz'\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "82px",
    "width": "338px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "354px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
