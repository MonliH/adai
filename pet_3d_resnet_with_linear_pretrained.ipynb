{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T01:38:37.283771Z",
     "start_time": "2020-02-18T01:38:36.679527Z"
    }
   },
   "source": [
    "# PET Residual Neural Network with Linear Layer Pretrianed\n",
    "This is the resnet model with a linear layer at the end; data is padded with black borders. Uses pretrained [medicalnet](https://github.com/Tencent/MedicalNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:58:04.996870Z",
     "start_time": "2020-03-08T11:58:03.678688Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "from sklearn import preprocessing\n",
    "from torchvision import transforms, utils\n",
    "import adabound\n",
    "import numpy as np\n",
    "\n",
    "import nibabel as nib\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:58:05.001755Z",
     "start_time": "2020-03-08T11:58:04.998952Z"
    }
   },
   "outputs": [],
   "source": [
    "#import MedicalNet as mn\n",
    "import os\n",
    "os.chdir(\"MedicalNet/\")\n",
    "import model as mn\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:58:05.027621Z",
     "start_time": "2020-03-08T11:58:05.018002Z"
    }
   },
   "outputs": [],
   "source": [
    "dir(mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:58:05.049409Z",
     "start_time": "2020-03-08T11:58:05.029094Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the GPU if there is one, otherwise CPU\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "CSV_DIR = \"./scores.csv\"\n",
    "DATA_DIR = \"../pet_data/\"\n",
    "\n",
    "MIN = 4401596\n",
    "MAX = 9233460"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T21:52:00.551756Z",
     "start_time": "2020-05-12T21:52:00.548764Z"
    }
   },
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:58:05.549352Z",
     "start_time": "2020-03-08T11:58:05.051304Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_DIR)\n",
    "norm_df = df[[\"mmse\", \"cdr\", \"ageAtEntry\"]]\n",
    "\n",
    "std_scale = preprocessing.StandardScaler().fit(norm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Management\n",
    "Handle CSV diagnosis/signs and get an iterator of brain scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:58:05.559365Z",
     "start_time": "2020-03-08T11:58:05.551649Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_scores(ID, date):\n",
    "    scores = []\n",
    "    for index, row in df[df[\"Subject\"].str.contains(ID)].iterrows():\n",
    "        cur_date = int(row[\"ADRC_ADRCCLINICALDATA ID\"].split(\"_\")[-1][1:])\n",
    "        if cur_date > date:\n",
    "            if cur_date > date:\n",
    "                if pd.isna(row[\"mmse\"]): row[\"mmse\"] = 30\n",
    "                if pd.isna(row[\"cdr\"]): row[\"cdr\"] = 0\n",
    "                data = {\n",
    "                    'mmse':  [row[\"mmse\"]],\n",
    "                    'cdr':  [row[\"cdr\"]],\n",
    "                    'ageAtEntry': [row[\"ageAtEntry\"]+cur_date/365]\n",
    "                }\n",
    "\n",
    "                curr_df = std_scale.transform(pd.DataFrame(data, columns=[\"mmse\", \"cdr\", \"ageAtEntry\"]))\n",
    "\n",
    "                scores.append((cur_date-date, curr_df[0][0], curr_df[0][1], curr_df[0][2]))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:58:05.610709Z",
     "start_time": "2020-03-08T11:58:05.582773Z"
    }
   },
   "outputs": [],
   "source": [
    "get_scores('OAS30001', 423)  # testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:58:06.254069Z",
     "start_time": "2020-03-08T11:58:06.243279Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_brains():\n",
    "    subjects = range(1, 11173)\n",
    "    for subject in subjects:\n",
    "        subject_id = str(subject).zfill(4)\n",
    "        path = f\"{DATA_DIR}sub-OAS3{subject_id}/\"\n",
    "        if os.path.isdir(path):\n",
    "            for session in os.listdir(path):\n",
    "                file = f\"{path}{session}/pet/sub-OAS3{subject_id}_{session}_acq-PIB_pet.nii.gz\"\n",
    "                if os.path.isfile(file):\n",
    "                    for score in get_scores(f\"OAS3{subject_id}\", int(session[5:])):\n",
    "                        yield (file, f\"OAS3{subject_id}\", int(session[5:])) + score\n",
    "                else:\n",
    "                    print(file)\n",
    "\n",
    "def list_brains():\n",
    "    subjects = range(1, 11173)\n",
    "    for subject in subjects:\n",
    "        subject_id = str(subject).zfill(4)\n",
    "        path = f\"{DATA_DIR}sub-OAS3{subject_id}/\"\n",
    "        if os.path.isdir(path):\n",
    "            for session in os.listdir(path):\n",
    "                file = f\"{path}{session}/pet/sub-OAS3{subject_id}_{session}_acq-PIB_pet.nii.gz\"\n",
    "                if os.path.isfile(file):\n",
    "                    yield (file, f\"OAS3{subject_id}\", int(session[5:]))\n",
    "                else:\n",
    "                    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset\n",
    "Create an iterable dataset with brain data inheriting from `torch.utils.data.Dataset`. Dataset len is `3594`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:58:06.670291Z",
     "start_time": "2020-03-08T11:58:06.664026Z"
    }
   },
   "outputs": [],
   "source": [
    "class BrainsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.brains = []\n",
    "        for brain_name in get_brains():\n",
    "            if get_scores(*brain_name[1:3]) != []:\n",
    "                self.brains.append(brain_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.brains)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = nib.load(self.brains[index][0])\n",
    "        return self.brains[index][3], self.brains[index][4], self.brains[index][5], self.brains[index][6], self.transform((data.get_fdata()+MIN)/MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T20:44:03.363326Z",
     "start_time": "2020-02-18T20:44:03.357375Z"
    }
   },
   "source": [
    "## Create Data Preprocessing and Cropping\n",
    "Crop images to (128, 128, 63, 24)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:58:07.564067Z",
     "start_time": "2020-03-08T11:58:07.558882Z"
    }
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, brain):\n",
    "        img = transform.resize(brain, self.output_size)\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return torch.from_numpy(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network\n",
    "Create a sparse cnn module inheriting from `torch.nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:58:08.275727Z",
     "start_time": "2020-03-08T11:58:08.257874Z"
    }
   },
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "\n",
    "    def forward(self, input):\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "\n",
    "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, resnet):\n",
    "        nn.Module.__init__(self)\n",
    "        self.num_classes = 64\n",
    "        self.resnet = resnet.to(DEVICE)\n",
    "        self.avg_pool = nn.AvgPool3d(3, stride=(3,3,3))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4096, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, self.num_classes),\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.num_classes*24+2, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            torch.nn.ELU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            torch.nn.ELU(),\n",
    "            nn.Linear(128, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            torch.nn.ELU(),\n",
    "            nn.Linear(16, 2)\n",
    "        ).to(DEVICE)\n",
    "        self.max_pool = nn.MaxPool3d(2, stride=(2, 2, 1))\n",
    "\n",
    "    def forward(self, brain, days_ahead, age):\n",
    "        c_out = self.resnet(self.max_pool(brain[:, None, :, :, :, 0]))\n",
    "        c_out = self.avg_pool(c_out).view(c_out.size(0), -1)\n",
    "        c_out = self.fc(c_out).to(DEVICE)\n",
    "        for i in range(brain.shape[-1]-1):\n",
    "            output = self.resnet(self.max_pool(brain[:, None, :, :, :, i]))\n",
    "            output = self.avg_pool(output).view(output.size(0), -1)\n",
    "            c_out = torch.cat([c_out, self.fc(output.view(output.size(0), -1))], 1)\n",
    "            del output\n",
    "        c_out = torch.cat([torch.stack([days_ahead, age]).permute(1, 0), c_out], 1).to(DEVICE)\n",
    "        r_out = torch.cuda.FloatTensor(self.linear(c_out)).to(DEVICE)\n",
    "        return r_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:58:08.644289Z",
     "start_time": "2020-03-08T11:58:08.629369Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, criterion_test, train_loader, test_loader, writer):\n",
    "    step_num = 0\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        print('-' * 10)\n",
    "        running_loss = 0\n",
    "        train_iter = iter(train_loader)\n",
    "        \n",
    "        for i, data_brains in enumerate(train_loader):\n",
    "            step_num += 1\n",
    "            scan = data_brains[4].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "            days_ahead = data_brains[0].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "            age = data_brains[3].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "            real_values = torch.stack([data_brains[1], data_brains[2]]).permute(1, 0).to(DEVICE)\n",
    "\n",
    "            outputs = model(scan, days_ahead, age)\n",
    "\n",
    "            loss = criterion(outputs, real_values)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 5 == 4:\n",
    "                print(f\"[{epoch + 1} {i + 1}] loss: {running_loss/5}\")\n",
    "                writer.add_scalar(\"Training Loss\", running_loss/5, step_num)\n",
    "                running_loss = 0\n",
    "\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    try:\n",
    "                        test_data = next(train_iter)\n",
    "                    except StopIteration:\n",
    "                        train_iter = iter(train_loader)\n",
    "                        test_data = next(train_iter)\n",
    "\n",
    "                    _scan = test_data[4].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "                    _days_ahead = test_data[0].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "                    _age = test_data[3].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "                    _real_values = torch.stack([test_data[1], test_data[2]]).permute(1, 0).to(DEVICE)\n",
    "                    _outputs = model(_scan, _days_ahead, _age)\n",
    "                    writer.add_scalar(\"Test Loss\", criterion_test(_outputs, _real_values), step_num)\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        print(\"Saving model\")\n",
    "        torch.save(model.state_dict(), f\"model_pretrained_cnn/new_model_{epoch}_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:58:09.557387Z",
     "start_time": "2020-03-08T11:58:09.546615Z"
    }
   },
   "outputs": [],
   "source": [
    "class Params():\n",
    "    def __init__(self):\n",
    "        self.model = \"resnet\"\n",
    "        self.model_depth = 34\n",
    "        self.input_W = 64\n",
    "        self.input_H = 64\n",
    "        self.input_D = 62\n",
    "        self.resnet_shortcut = \"A\"\n",
    "        self.no_cuda = False\n",
    "        self.gpu_id = [0]\n",
    "        self.n_seg_classes = 128\n",
    "        self.phase = \"train\"\n",
    "        self.pretrain_path = os.getcwd()+\"/MedicalNet/pretrain/resnet_34_23dataset.pth\"\n",
    "        self.new_layer_names = ['upsample1', 'cmp_layer3', 'upsample2', 'cmp_layer2', 'upsample3', 'cmp_layer1', 'upsample4', 'cmp_conv1', 'conv_seg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T03:08:31.031609Z",
     "start_time": "2020-03-06T03:08:31.026928Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T03:09:27.417597Z",
     "start_time": "2020-03-06T03:08:31.773566Z"
    }
   },
   "outputs": [],
   "source": [
    "scale = Rescale((128, 128, 63, 24))\n",
    "composed = transforms.Compose([scale, ToTensor()])\n",
    "\n",
    "dataset = BrainsDataset(composed)\n",
    "\n",
    "NUM_INSTANCES = len(dataset)\n",
    "TEST_RATIO = 0.2\n",
    "TEST_SIZE = int(NUM_INSTANCES * TEST_RATIO)\n",
    "TRAIN_SIZE = NUM_INSTANCES - TEST_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T03:09:27.446943Z",
     "start_time": "2020-03-06T03:09:27.419151Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = torch.utils.data.random_split(dataset, (TRAIN_SIZE, TEST_SIZE))\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T03:09:27.457837Z",
     "start_time": "2020-03-06T03:09:27.454297Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-06T03:08:33.549Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "model, _ = mn.generate_model(Params())\n",
    "\n",
    "model = Net(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())  \n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion_test = torch.nn.MSELoss()\n",
    "writer = SummaryWriter()\n",
    "\n",
    "loss = train(model, optimizer, criterion, criterion_test, train_loader, test_loader, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T02:47:10.108419Z",
     "start_time": "2020-03-06T02:45:47.063Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T13:22:25.616Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, data_brains in enumerate(train_loader):\n",
    "    # data_brains = days_ahead, mmse, cdr, age, scan\n",
    "    # output = (days ahead, mmse, cdr)\n",
    "    # scan = [x,y,z,t]\n",
    "    scan = data_brains[4].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "    days_ahead = data_brains[0].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "    # mmse = mmse.type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "    # cdr = cdr.type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "    age = data_brains[3].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "\n",
    "    writer.add_graph(model, (scan, days_ahead, age))\n",
    "    break\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T13:09:12.468063Z",
     "start_time": "2020-03-05T13:09:11.434653Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, f\"model_cnn_normal_final_new.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T13:22:25.632Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T13:22:25.638Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, data_brains in enumerate(train_loader):\n",
    "        # data_brains = days_ahead, mmse, cdr, age, scan\n",
    "        # output = (days ahead, mmse, cdr)\n",
    "        # scan = [x,y,z,t]\n",
    "        scan = data_brains[4].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "        days_ahead = data_brains[0].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "        # mmse = mmse.type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "        # cdr = cdr.type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "        age = data_brains[3].type(torch.cuda.FloatTensor).to(DEVICE)\n",
    "        real_values = torch.stack([data_brains[1], data_brains[2]]).permute(1, 0).to(DEVICE)\n",
    "\n",
    "        # print(scan.shape)\n",
    "        model.eval()\n",
    "        outputs = model(scan[None, 0], days_ahead[None, 0], age[None, 0])\n",
    "        print(f\"Outputs: {outputs}\")\n",
    "        print(f\"Real Values: {real_values}\")\n",
    "\n",
    "        data_pred = {\n",
    "            'mmse':  [outputs[0][0]],\n",
    "            'cdr':  [outputs[0][1]]\n",
    "        }\n",
    "\n",
    "        data_pred = std_scale.inverse_transform(pd.DataFrame(data_pred, columns=[\"mmse\", \"cdr\", \"ageAtEntry\"]))\n",
    "\n",
    "        data_real = {\n",
    "            'mmse':  [real_values[0][0]],\n",
    "            'cdr':  [real_values[0][1]]\n",
    "        }\n",
    "\n",
    "        data_real = std_scale.inverse_transform(pd.DataFrame(data_real, columns=[\"mmse\", \"cdr\", \"ageAtEntry\"]))\n",
    "    \n",
    "        print(data_pred)\n",
    "        print(data_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T13:22:25.650Z"
    }
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Dimensions of Data\n",
    "Here we get the dimensions of the data to be cropped\n",
    "\n",
    "This distribution goes something like this:\n",
    "\n",
    "```\n",
    "{(128, 128, 63, 51): 88,\n",
    " (128, 128, 63, 25): 60,\n",
    " (128, 128, 63, 24): 11,\n",
    " (128, 128, 109, 26): 549,\n",
    " (128, 128, 63, 52): 76,\n",
    " (128, 128, 63, 53): 41,\n",
    " (128, 128, 63, 26): 51,\n",
    " (128, 128, 63, 50): 34,\n",
    " (256, 256, 127, 26): 5,\n",
    " (128, 128, 63, 41): 2,\n",
    " (128, 128, 2592): 1,\n",
    " (128, 128, 74, 25): 1,\n",
    " (128, 128, 63, 49): 11,\n",
    " (256, 256, 127): 2,\n",
    " (128, 128, 63, 23): 4,\n",
    " (128, 128, 2832): 4,\n",
    " (128, 128, 63, 34): 2,\n",
    " (128, 128, 47, 49): 2,\n",
    " (128, 128, 63, 48): 1,\n",
    " (128, 128, 109, 4): 1,\n",
    " (128, 128, 2827): 1,\n",
    " (128, 128, 47, 50): 1,\n",
    " (128, 128, 109, 20): 2,\n",
    " (128, 128, 47, 52): 1,\n",
    " (128, 128, 109, 17): 1,\n",
    " (128, 128, 109, 6): 1,\n",
    " (128, 128, 63, 20): 1,\n",
    " (128, 128, 47, 51): 1,\n",
    " (128, 128, 63, 45): 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-08T11:58:16.883Z"
    }
   },
   "outputs": [],
   "source": [
    "dimensions = {}\n",
    "\n",
    "for brain in get_brains():\n",
    "    shape = nib.load(brain[0]).get_fdata().shape\n",
    "    if shape in dimensions:\n",
    "        dimensions[shape] += 1\n",
    "    else:\n",
    "        dimensions[shape] = 1\n",
    "\n",
    "print(dimensions)\n",
    "\n",
    "nums = 0\n",
    "for num in dimensions.values():\n",
    "    nums+=num\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-08T11:58:33.470Z"
    }
   },
   "outputs": [],
   "source": [
    "smallest, largest = 0, 0\n",
    "\n",
    "for brain in list_brains():\n",
    "    data = nib.load(brain[0]).get_fdata()\n",
    "    \n",
    "    if np.min(data) < smallest:\n",
    "        smallest = np.min(data)\n",
    "    if int(np.max(data)) > largest:\n",
    "        largest = np.max(data)\n",
    "\n",
    "(smallest, largest)\n",
    "# should be (-4401596.0, 4831864.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T13:22:25.662Z"
    }
   },
   "outputs": [],
   "source": [
    "for brain in get_brains():\n",
    "    shape = nib.load(brain[0]).get_fdata().shape\n",
    "    if len(shape) == 3:\n",
    "        print(brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-02T13:22:25.665Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_data = ['../data/sub-OAS30065/ses-d0553/pet/sub-OAS30065_ses-d0553_acq-PIB_pet.nii.gz',\n",
    "'../data/sub-OAS30229/ses-d0101/pet/sub-OAS30229_ses-d0101_acq-PIB_pet.nii.gz',\n",
    "'../data/sub-OAS30253/ses-d3948/pet/sub-OAS30253_ses-d3948_acq-PIB_pet.nii.gz',\n",
    "'../data/sub-OAS30332/ses-d0091/pet/sub-OAS30332_ses-d0091_acq-PIB_pet.nii.gz',\n",
    "'../data/sub-OAS30472/ses-d1278/pet/sub-OAS30472_ses-d1278_acq-PIB_pet.nii.gz',\n",
    "'../data/sub-OAS30498/ses-d0120/pet/sub-OAS30498_ses-d0120_acq-PIB_pet.nii.gz',\n",
    "'../data/sub-OAS30588/ses-d1639/pet/sub-OAS30588_ses-d1639_acq-PIB_pet.nii.gz',\n",
    "'../data/sub-OAS30896/ses-d1601/pet/sub-OAS30896_ses-d1601_acq-PIB_pet.nii.gz'\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "82px",
    "width": "338px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "354px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
